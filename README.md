# Moteur de recherche web minimaliste en Python

Un moteur de recherche modulaire dÃ©veloppÃ© en Python, utilisant SQLite pour le stockage persistant des donnÃ©es.

## ğŸ“‹ AperÃ§u

Ce projet implÃ©mente un moteur de recherche complet avec les composants suivants :
- Un crawler web pour collecter les documents
- Un indexeur pour traiter et organiser les donnÃ©es textuelles
- Un moteur de recherche avec algorithme TF-IDF pour le classement des rÃ©sultats
- Une interface web intuitive dÃ©veloppÃ©e avec Flask

L'architecture est conÃ§ue pour Ãªtre modulaire, extensible et dÃ©montrer les principes fondamentaux des moteurs de recherche.

## ğŸ› ï¸ FonctionnalitÃ©s

- **Crawling web** : Explore le web Ã  partir d'URLs de dÃ©part, extrait le contenu textuel et les liens
- **Indexation** : PrÃ©traitement linguistique (tokenisation, suppression des mots vides, stemming) et crÃ©ation d'un index inversÃ©
- **Recherche** : Algorithme TF-IDF pour le classement des rÃ©sultats par pertinence
- **Persistance** : Stockage efficace des documents et de l'index dans une base de donnÃ©es SQLite
- **Interface web** : Interface utilisateur simple et rÃ©active avec AJAX

## ğŸ”§ PrÃ©requis

- Python 3.7+
- BibliothÃ¨ques Python :
  - requests
  - beautifulsoup4
  - nltk
  - flask

## ğŸ“¥ Installation

1. Clonez le dÃ©pÃ´t :
```bash
git clone https://github.com/votrenom/moteur-recherche-python.git
cd moteur-recherche-python
```

2. Installez les dÃ©pendances :
```bash
pip install -r requirements.txt
```

3. TÃ©lÃ©chargez les ressources NLTK :
```python
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

## ğŸš€ Utilisation

### Structure du projet

```
moteur_recherche/
â”œâ”€â”€ db/
â”‚   â””â”€â”€ search_engine.db
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ crawler.py
â”œâ”€â”€ indexer/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ indexer.py
â”œâ”€â”€ searcher/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ searcher.py
â”œâ”€â”€ web_ui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ search.html
â”œâ”€â”€ db_manager.py
â”œâ”€â”€ main.py
â””â”€â”€ requirements.txt
```

### Commandes

1. **Crawler des sites web** :
```bash
python main.py --crawl --urls https://fr.wikipedia.org/wiki/Python_(langage) --max-pages 20
```

2. **Indexer les documents** :
```bash
python main.py --index
```

3. **Lancer l'interface web** :
```bash
python main.py
```

4. **ExÃ©cuter le processus complet** :
```bash
python main.py --crawl --index --urls https://fr.wikipedia.org/wiki/Python_(langage) --max-pages 50
```

Une fois lancÃ©, accÃ©dez Ã  l'interface web Ã  l'adresse http://localhost:5000

## ğŸ§® Aspects mathÃ©matiques

Le moteur utilise plusieurs concepts mathÃ©matiques :

### TF-IDF (Term Frequency-Inverse Document Frequency)

La formule utilisÃ©e pour calculer la pertinence d'un document :

$$\text{Score}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)$$

OÃ¹ :
- $\text{TF}(t,d) = \frac{\text{nombre d'occurrences de } t \text{ dans } d}{\text{nombre total de termes dans } d}$
- $\text{IDF}(t) = \log\left(\frac{\text{nombre total de documents}}{\text{nombre de documents contenant } t}\right)$

### ModÃ¨le vectoriel

Les documents et requÃªtes sont reprÃ©sentÃ©s comme des vecteurs dans un espace multidimensionnel, oÃ¹ chaque dimension correspond Ã  un terme unique.

## ğŸ” Fonctionnement dÃ©taillÃ©

### DatabaseManager (db_manager.py)
GÃ¨re la base de donnÃ©es SQLite, crÃ©e les tables et fournit les connexions.

### WebCrawler (crawler/crawler.py)
Explore le web en suivant les liens et stocke les documents dans la base de donnÃ©es.

### Indexer (indexer/indexer.py)
PrÃ©traite le texte des documents et construit l'index inversÃ©.

### SearchEngine (searcher/searcher.py)
Traite les requÃªtes et renvoie les rÃ©sultats classÃ©s par pertinence.

### WebInterface (web_ui/app.py)
Interface utilisateur pour interagir avec le moteur de recherche.

## ğŸ“ˆ Performances

Sur un corpus de test de 50 pages :
- Temps de crawling : ~2 minutes
- Temps d'indexation : quelques secondes
- Temps de rÃ©ponse aux requÃªtes : < 100ms

## ğŸ”„ AmÃ©lioration possibles

- ImplÃ©mentation d'un algorithme PageRank pour amÃ©liorer le classement
- Gestion des fautes d'orthographe dans les requÃªtes
- Analyse sÃ©mantique pour mieux comprendre le contexte
- Interface d'administration pour gÃ©rer l'indexation
- ParallÃ©lisation du crawler pour amÃ©liorer les performances

## ğŸ“š Ressources

- [ThÃ©orie des moteurs de recherche](https://en.wikipedia.org/wiki/Search_engine_(computing))
- [TF-IDF et recherche d'information](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
- [Documentation de NLTK](https://www.nltk.org/)
- [Flask Documentation](https://flask.palletsprojects.com/)

## ğŸ“„ Licence

Ce projet est sous licence MIT - voir le fichier LICENSE pour plus de dÃ©tails.
